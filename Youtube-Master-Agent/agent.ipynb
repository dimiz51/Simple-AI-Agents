{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import nest_asyncio\n",
    "import uuid\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import yt_dlp\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, ToolCallResult, AgentStream\n",
    "from typing import List\n",
    "\n",
    "# For async tool calls in a notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_API_KEY\")\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\", token=HF_TOKEN)\n",
    "\n",
    "# Whisper HFInference API\n",
    "WHISPER_HF_API_URL = \"https://api-inference.huggingface.co/models/openai/whisper-large-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tools for transcription and video fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_youtube_audio(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a YouTube video and extracts its audio as an MP3 file, storing it in a temporary directory.\n",
    "    Returns the path to the downloaded MP3 file and the title of the video.\n",
    "    \"\"\"\n",
    "    temp_dir = Path(tempfile.gettempdir())  # Use system temp directory\n",
    "    audio_file_id = str(uuid.uuid4())\n",
    "    output_dir = str(temp_dir / f'{audio_file_id}')\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': str(output_dir),\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    loop = asyncio.get_running_loop()\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info_dict = await loop.run_in_executor(None, ydl.extract_info, url)\n",
    "            video_title = info_dict.get('title', 'Unknown Title')\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "    \n",
    "    audio_file = f\"{output_dir}.mp3\"\n",
    "    response = f\"Audio file: {audio_file}, Title: {video_title}\"\n",
    "    return response\n",
    "\n",
    "async def transcribe_audio(audio_file: str) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously transcribes an MP3 audio file into text using the Hugging Face Inference API.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load HF token\n",
    "    HF_TOKEN = os.getenv(\"HF_API_KEY\")\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "\n",
    "    # Verify audio file ends with .mp3 and remove duplicate '.mp3' \n",
    "    if not audio_file.endswith('.mp3'):\n",
    "        return \"Audio file must be in MP3 format.\"\n",
    "\n",
    "    # Using aiohttp for non-blocking HTTP requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with open(audio_file, \"rb\") as f:\n",
    "            audio_data = f.read()\n",
    "\n",
    "        # Send the audio file for transcription\n",
    "        async with session.post(WHISPER_HF_API_URL, headers=headers, data=audio_data) as response:\n",
    "            result = await response.json()\n",
    "\n",
    "    # Return transcription text or error message\n",
    "    return result.get(\"text\", \"Transcription failed for {audio_file}. Either no word was detected or there is something wrong with the audio file.\")\n",
    "\n",
    "async def video_information_response(summaries: List[str], titles: List[str], topics: List[str], urls: List[str]) -> str:\n",
    "    \"\"\" Formats the response with the summaries, titles, topics, and URLs for all videos in a multi-line string.\"\"\"\n",
    "    # Ensure all lists have the same length\n",
    "    if not (len(summaries) == len(titles) == len(topics) == len(urls)):\n",
    "        return \"Error: Input lists must have the same length.\"\n",
    "\n",
    "    # Construct a formatted string with a neat layout\n",
    "    formatted_response = []\n",
    "    \n",
    "    for i in range(len(summaries)):\n",
    "        formatted_response.append(f\"Video {i+1}:\")\n",
    "        formatted_response.append(f\"  Title: {titles[i]}\")\n",
    "        formatted_response.append(f\"  URL: {urls[i]}\")\n",
    "        formatted_response.append(f\"  Topic: {topics[i]}\")\n",
    "        formatted_response.append(f\"  Summary: {summaries[i]}\")\n",
    "        formatted_response.append(\"\")  # Blank line between videos\n",
    "\n",
    "    # Join all lines into a single string with line breaks\n",
    "    return \"\\n\".join(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Agent workflow and test reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    tools_or_functions=[transcribe_audio, download_youtube_audio, video_information_response],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are an AI assistant that can use tools to transcribe audio from YouTube videos, create summaries of the video's content's, provide contextual information of the videos and answer related questions \"\n",
    "                  \"related to the videos. You will NEVER respond to tasks unrelated to your role, instead alert the user that you are unable to perform the task. \"\n",
    "                  \"ALWAYS remember to provide summaries, titles, video topics, and video url's for all videos that the user asks for if you successfully transcribe the audio. \"\n",
    "                  \"If some video failed to be transcribed, reply only with the succesfull results and a detailed explanation of the errors for the unsuccessful videos. \"\n",
    "                  \"Always use the available tools to format the response.\",\n",
    ")\n",
    "\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "handler = agent.run(\n",
    "    \"I want summaries for the following videos: \"\n",
    "    \"1. https://www.youtube.com/watch?v=rEDzUT3ymw4 \"\n",
    "    \"2. https://www.youtube.com/watch?v=epVW0_iVBX8\",\n",
    "    ctx=ctx\n",
    "    )\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(\"\")\n",
    "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
    "    elif isinstance(ev, AgentStream):\n",
    "        print(ev.delta, end=\"\", flush=True)\n",
    "\n",
    "resp = await handler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n*******************Agent response********************\")\n",
    "print(resp.response.blocks[-1].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
